# -*- coding: utf-8 -*-
"""CIS519_Final_Project[FINAL VERSION].ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WtDqWcJTbs5LKQdXNxYMu4GJxZ27w6V1
"""

from google.colab import drive
drive.mount('/content/drive')

from sklearn.metrics import accuracy_score

def file_to_list(file_path):
    arr = []
    with open(file_path, 'r') as f:
        for line in f:
            arr.append(line.strip('\n'))
    return arr

import pathlib
pathlib.Path().absolute()
expected_path = "/content/drive/MyDrive/project/NEI_labels/expected/"

'''
  Trained on train_6
'''
path = "/content/drive/MyDrive/project/NEI_labels/train_6/"
acc = accuracy_score(file_to_list(f"{path}dev_6_actual_labels.txt"),
file_to_list(f"{expected_path}dev_6_expected_labels.txt"))
print(f"dev_6 on train_6: {acc}")
acc = accuracy_score(file_to_list(f"{path}test_6_actual_labels.txt"),
file_to_list(f"{expected_path}test_6_expected_labels.txt"))
print(f"test_6 on train_6: {acc}")
acc = accuracy_score(file_to_list(f"{path}dev_12_actual_labels.txt"),
file_to_list(f"{expected_path}dev_12_expected_labels.txt"))
print(f"dev_12 on train_6: {acc}")
acc = accuracy_score(file_to_list(f"{path}test_12_actual_labels.txt"),
file_to_list(f"{expected_path}test_12_expected_labels.txt"))
print(f"test_12 on train_6: {acc}")
acc = accuracy_score(file_to_list(f"{path}dev_18_actual_labels.txt"),
file_to_list(f"{expected_path}dev_18_expected_labels.txt"))
print(f"dev_18 on train_6: {acc}")
acc = accuracy_score(file_to_list(f"{path}test_18_actual_labels.txt"),
file_to_list(f"{expected_path}test_18_expected_labels.txt"))
print(f"test_18 on train_6: {acc}")

'''
  Trained on train_12
'''
path = "/content/drive/MyDrive/project/NEI_labels/train_12/"
acc = accuracy_score(file_to_list(f"{path}dev_6_actual_labels.txt"),
file_to_list(f"{expected_path}dev_6_expected_labels.txt"))
print(f"dev_6 on train_12: {acc}")
acc = accuracy_score(file_to_list(f"{path}test_6_actual_labels.txt"),
file_to_list(f"{expected_path}test_6_expected_labels.txt"))
print(f"test_6 on train_12: {acc}")
acc = accuracy_score(file_to_list(f"{path}dev_12_actual_labels.txt"),
file_to_list(f"{expected_path}dev_12_expected_labels.txt"))
print(f"dev_12 on train_12: {acc}")
acc = accuracy_score(file_to_list(f"{path}test_12_actual_labels.txt"),
file_to_list(f"{expected_path}test_12_expected_labels.txt"))
print(f"test_12 on train_12: {acc}")
acc = accuracy_score(file_to_list(f"{path}dev_18_actual_labels.txt"),
file_to_list(f"{expected_path}dev_18_expected_labels.txt"))
print(f"dev_18 on train_12: {acc}")
acc = accuracy_score(file_to_list(f"{path}test_18_actual_labels.txt"),
file_to_list(f"{expected_path}test_18_expected_labels.txt"))
print(f"test_18 on train_12: {acc}")

'''
  Trained on train_18
'''
path = "/content/drive/MyDrive/project/NEI_labels/train_18/"
acc = accuracy_score(file_to_list(f"{path}dev_6_actual_labels.txt"),
file_to_list(f"{expected_path}/dev_6_expected_labels.txt"))
print(f"dev_6 on train_18: {acc}")
acc = accuracy_score(file_to_list(f"{path}test_6_actual_labels.txt"),
file_to_list(f"{expected_path}test_6_expected_labels.txt"))
print(f"test_6 on train_18: {acc}")
acc = accuracy_score(file_to_list(f"{path}dev_12_actual_labels.txt"),
file_to_list(f"{expected_path}dev_12_expected_labels.txt"))
print(f"dev_12 on train_18: {acc}")
acc = accuracy_score(file_to_list(f"{path}test_12_actual_labels.txt"),
file_to_list(f"{expected_path}test_12_expected_labels.txt"))
print(f"test_12 on train_18: {acc}")
acc = accuracy_score(file_to_list(f"{path}dev_18_actual_labels.txt"),
file_to_list(f"{expected_path}dev_18_expected_labels.txt"))
print(f"dev_18 on train_18: {acc}")
acc = accuracy_score(file_to_list(f"{path}test_18_actual_labels.txt"),
file_to_list(f"{expected_path}test_18_expected_labels.txt"))
print(f"test_18 on train_18: {acc}")

import json
json_path = "/content/drive/MyDrive/project/pickle_data.json"
json_file = open(json_path)
json_data = json.load(json_file)

some_data = {k: json_data[k] for k in list(json_data)[:10]}
print(some_data.keys())

"""
manually select examples of each label
"""
person_titles = ["God", "Jesus", "Barack_Obama", "George_W._Bush", "Adolf_Hitler", "Michael_Jackson", "William_Shakespeare", "Elvis_Presley", "John_F._Kennedy", "Leonardo_da_Vinci", "Aristotle", "Albert_Einstein"]
norp_titles = ["Republican_Party_(United_States)", "Christianity", "Judaism", "Democratic_Party_(United_States)", "Confederate_States_of_America"]
fac_titles = ["White_House", "Empire_State_Building", "Eiffel_Tower", "Taj_Mahal", "O'Hare", "Colosseum"]
org_titles = ["Flickr", "Wikipedia", "Microsoft", "The_New_York_Times", "Facebook", "Google", "Harvard_University", "NASA", "United_States_Navy", "British_Army", "International_Space_Station", "United_States_Air_Force"]
gpe_titles = ["United_States", "India", "Canada", "New_York_City", "Europe", "London", "Japan", "France", "United_Kingdom", "Germany", "Australia", "England", "California", "China", "Italy"]
loc_titles = ["Moon", "Indian_Ocean", "Pacific_Ocean", "Atlantic_Ocean", "Mississippi_River"]
product_titles = ["Microsoft_Windows", "Xbox_360", "PlayStation_3", "IPhone", "Mobile_phone", "Grammy_Award", "PlayStation_2", "Nintendo_DS"]
event_titles = ["World_War_II", "World_War_I", "American_Civil_War", "2012_Summer_Olympics", "Hurricane_Katrina", "American_Revolutionary_War", "September_11_attacks", "Olympic_Games"]
work_of_art_titles = ["Rock_music", "Pop_music", "The_Beatles", "Jazz", "Harry_Potter", "Star_Wars", "Lost_(TV_series)", "Heavy_metal_music", "Punk_rock", "Disney_Channel", "The_Lord_of_the_Rings"]
law_titles = ["First_Amendment_to_the_United_States_Constitution", "United_States_Constitution"]
language_titles = ["English_language", "Greek_language", "French_language", "Arabic_language", "Sanskrit", "Spanish_language", "German_language", "Korean_language"]
date_titles = ["2007", "2006", "2008", "2005", "2004", "2003", "2001", "2000"]
time_titles = ["Hour", "Minute", "Second", "Nanosecond", "Picosecond"]
percent_titles = ["25", "More", "82", "200", "100"]
money_titles = ["Dollar", "Euro", "Yen", "Renminbi", "rupee", "Franc"]
quantity_titles = ["Liter", "Inch", "Ounce", "Centimeter", "Gram"]
ordinal_titles = ["First", "Second", "Third", "Fourth", "Fifth", "Sixth", "seventh", "eighth", "ninth"]
cardinal_titles = ["Grammatical_number", "One", "Two", "Three", "Four", "Five", "Six", "Seven"]

def get_sentences_from_titles(json_data, titles):
  '''
  returns a list of sentences from a list of titles present in json_data
  json_data -- mapping between title and list of sentences
  titles -- a list of relevant titles we want the sentences for
  '''
  sentence_groups = [json_data[x].split(".|||") for x in titles]
  flattened_sentences = [val for sublist in sentence_groups for val in sublist]
  return flattened_sentences

def get_sentences_from_titles(json_data, titles):
  '''
  returns a list of sentences from a list of titles present in json_data
  json_data -- mapping between title and list of sentences
  titles -- a list of relevant titles we want the sentences for
  '''
  sentence_tuples = []
  for x in titles:
    sentences = json_data[x].split(".|||")[:20]
    for sentence in sentences:
      i = 0
      words = sentence.split(" ")
      if x in words:
        word_index = words.index(x)
        sentence_tuples.append((sentence, word_index))
        i += 1
  return sentence_tuples

person_sentences = get_sentences_from_titles(json_data, person_titles)
norp_sentences = get_sentences_from_titles(json_data, norp_titles)
fac_sentences = get_sentences_from_titles(json_data, fac_titles)
org_sentences = get_sentences_from_titles(json_data, org_titles)
gpe_sentences = get_sentences_from_titles(json_data, gpe_titles)
loc_sentences = get_sentences_from_titles(json_data, loc_titles)
product_sentences = get_sentences_from_titles(json_data, product_titles)
event_sentences = get_sentences_from_titles(json_data, event_titles)
work_of_art_sentences = get_sentences_from_titles(json_data, work_of_art_titles)
law_sentences = get_sentences_from_titles(json_data, law_titles)
language_sentences = get_sentences_from_titles(json_data, language_titles)
date_sentences = get_sentences_from_titles(json_data, date_titles)
time_sentences = get_sentences_from_titles(json_data, time_titles)
percent_sentences = get_sentences_from_titles(json_data, percent_titles)
money_sentences = get_sentences_from_titles(json_data, money_titles)
quantity_sentences = get_sentences_from_titles(json_data, quantity_titles)
ordinal_sentences = get_sentences_from_titles(json_data, ordinal_titles)
cardinal_sentences = get_sentences_from_titles(json_data, cardinal_titles)

!pip install transformers

from transformers import RobertaTokenizer, RobertaForTokenClassification
import torch

tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
model = RobertaForTokenClassification.from_pretrained('roberta-base')

tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
model = RobertaForTokenClassification.from_pretrained('roberta-base')

from itertools import zip_longest
import numpy as np

def compute_avg_vector(sentences):
  """
  goal of this function was to calculate the average vector from a list of sentences
  """
  sum_vector = [] 
  for s in sentences:
    sentence, title_index = s
    inputs = tokenizer(sentence, return_tensors="pt")
    labels = torch.tensor([1] * inputs["input_ids"].size(1)).unsqueeze(0)  # Batch size 1
    outputs = model(**inputs, labels=labels, output_hidden_states=True)
    embeddings_you_need = outputs.hidden_states[-1]
    if (title_index >= len(embeddings_you_need.detach().numpy()[0])):
      continue
    embeddings_arr = embeddings_you_need.detach().numpy()[0][title_index]

    temp_sum = []
    if len(sum_vector) < len(embeddings_arr):
      temp_sum = embeddings_arr.copy()
      temp_sum[:len(sum_vector)] += sum_vector
    else:
      temp_sum = sum_vector.copy()
      temp_sum[:len(embeddings_arr)] += embeddings_arr
    sum_vector = temp_sum
  return [(float(x) / len(sentences)) for x in sum_vector] # this statement fails

def gather_all_vectors(sentences):
  """
  goal of this function was to calculate the average vector from a list of sentences
  it is not going well
  TODO: fix the following issues
  1. + on 2 lists is not vectorwise (turn into numpy array? that sounds stupid tbh)
  2. idk how to just get the vector from outputs
  3. there's prob something wrong with the vector
  """
  sum_vector = [] 
  for s in sentences:
    sentence, title_index = s
    inputs = tokenizer(sentence, return_tensors="pt")
    labels = torch.tensor([1] * inputs["input_ids"].size(1)).unsqueeze(0)  # Batch size 1
    outputs = model(**inputs, labels=labels, output_hidden_states=True)
    embeddings_you_need = outputs.hidden_states[-1]
    embeddings_arr = embeddings_you_need.detach().numpy()[0][title_index]
    sum_vector.append(embeddings_arr)
  return sum_vector

person_vector = compute_avg_vector(person_sentences)
norp_vector = compute_avg_vector(norp_sentences)
fac_vector = compute_avg_vector(fac_sentences)
org_vector = compute_avg_vector(org_sentences)
gpe_vector = compute_avg_vector(gpe_sentences)
loc_vector = compute_avg_vector(loc_sentences)
product_vector = compute_avg_vector(product_sentences)
event_vector = compute_avg_vector(event_sentences)
work_of_art_vector = compute_avg_vector(work_of_art_sentences)
law_vector = compute_avg_vector(law_sentences)
language_vector = compute_avg_vector(language_sentences)
date_vector = compute_avg_vector(date_sentences)
time_vector = compute_avg_vector(time_sentences)
percent_vector = compute_avg_vector(percent_sentences)
money_vector = compute_avg_vector(money_sentences)
quantity_vector = compute_avg_vector(quantity_sentences)
ordinal_vector = compute_avg_vector(ordinal_sentences)
cardinal_vector = compute_avg_vector(cardinal_sentences)

label_vectors = {
    "PER": person_vector,
    "NORP": norp_vector,
    "FAC": fac_vector,
    "ORG": org_vector,
    "GPE": gpe_vector,
    "LOC": loc_vector,
    "PRODUCT": product_vector,
    "EVENT": event_vector,
    "WORK_OF_ART": work_of_art_vector,
    "LAW": law_vector,
    "LANGUAGE": language_vector,
    "DATE": date_vector,
    "TIME": time_vector,
    "PERCENT": percent_vector,
    "MONEY": money_vector,
    "QUANTITY": quantity_vector,
    "ORDINAL": ordinal_vector,
    "CARDINAL": cardinal_vector
}

# write the vectors (averages) to a json file
import json
path_to_json_vectors = "/content/drive/MyDrive/project/NER/label_vectors.json"
with open(path_to_json_vectors, "w") as outfile:  
    json.dump(label_vectors, outfile)

"""## After creating the json file with the average embeddings, proceed here


"""

import json
path_to_json_vectors = "/content/drive/MyDrive/project/NER/label_vectors.json"
label_vectors = {}
with open(path_to_json_vectors) as json_file:
    label_vectors = json.load(json_file)

from numpy import dot
from numpy.linalg import norm
def find_most_similar(label_vectors, to_classify):
  """
  label_vectors -- dictionary of labels mapped to vectors
  """
  most_similar_label = "PERSON"
  highest_cos_sim = 0
  for label, vector in label_vectors.items():
    cos_sim = dot(vector, to_classify) / (norm(to_classify)*norm(vector))
    if (cos_sim > highest_cos_sim):
      most_similar_label = label
      highest_cos_sim = cos_sim
  return most_similar_label, highest_cos_sim

import re
def load_and_parse_tokens_compiled(file_path):
    """
    Loads and separates tokens into one compiled array
    takes in one of the given files and returns a list of tokens with JUST the words (assumed at index 5)
    should be using this on the GIVEN files
    """
    tokens = []
    with open(file_path, 'r') as f:
        for line in f:
            split = re.split(r'\t+', line)
            if (len(split) >= 6):
                tokens.append(split[5])
    return tokens

def get_sentences_from_tokens(tokens):
  """
  returns a list of sentences from the tokens
  tokens -- list of tokens
  """
  all_sentences = []
  curr_sentence = ""
  for t in tokens:
    if t == ".":
      curr_sentence = curr_sentence[:-1]
      curr_sentence += t
      all_sentences.append(curr_sentence)
      curr_sentence = ""
    else:
      curr_sentence += t
      curr_sentence += " "
  return all_sentences

def get_sentence_position(tokens):
  """
  takes in a list of tokens
  returns a list of tuples with (word, sentence, position)
  tokens -- list of tokens (from load_and_parse_tokens_compiled)
  """
  all_tuples = []
  curr_sentence = ""
  for t in tokens:
    if t == ".":
      # curr_sentence = curr_sentence[:-1]
      curr_sentence += t # 
      # all_sentences.append(curr_sentence)
      for index, word in enumerate(curr_sentence.split(" ")):
        all_tuples.append((word, curr_sentence, index))
      curr_sentence = ""
    else:
      curr_sentence += t
      curr_sentence += " "
  return all_tuples

def file_to_list(file_path):
  """
  ASSUMES FILE CONTAINS INTS
  """
  arr = []
  with open(file_path, 'r') as f:
      for line in f:
          arr.append(int(line.strip('\n')))
  return arr

def list_to_file(items, file_path): 
  with open(file_path, 'w') as f:
      for item in items:
          f.write("%s\n" % item)

path_to_test_nei = "/content/drive/MyDrive/project/NEI_labels/train_18/test_18_actual_labels.txt" # actual labels
nei_labels = file_to_list(path_to_test_nei)

sentence_embeddings_map = {}
def get_vector_from_sentence(word, sentence, index):
  """
  WITH CACHING
  returns a vector for the word within a sentence
  to be called by get_labels_from_tuple_list
  word: str we are trying to classify
  sentence: str that the word is in
  index: index of the word in the sentence
  """
  embeddings_arr = []
  if sentence not in sentence_embeddings_map:
    sentence_embeddings_map.clear()
    inputs = tokenizer(sentence, return_tensors="pt")
    labels = torch.tensor([1] * inputs["input_ids"].size(1)).unsqueeze(0)  # Batch size 1
    outputs = model(**inputs, labels=labels, output_hidden_states=True)
    embeddings_you_need = outputs.hidden_states[-1]
    sentence_embeddings_map[sentence] = embeddings_you_need # cache the value
    return embeddings_you_need.detach().numpy()[0][index]
  else:
    embeddings_you_need = sentence_embeddings_map[sentence]
    return embeddings_you_need.detach().numpy()[0][index]

label_mappings = {
	"O": 0, # not an entity
	"PER": 1,
	"NORP": 2,
	"FAC": 3,
	"ORG": 4,
	"GPE": 5,
	"LOC": 6,
	"PRODUCT": 7,
	"EVENT": 8,
	"WORK_OF_ART": 9,
	"LAW": 10,
	"LANGUAGE": 11,
	"DATE": 12,
	"TIME": 13,
	"PERCENT": 14,
	"MONEY": 15,
	"QUANTITY": 16,
	"ORDINAL": 17,
	"CARDINAL": 18
}

def get_labels_from_tuple_list(tuple_list, nei_labels):
  """
  for use after get_sentence_position
  """
  arr = []
  for ind, item in enumerate(tuple_list):
    # not entity, put 0
    if (nei_labels[ind] == '0'):
      arr.append(0)
      continue
    w, s, i = item
    embeddings = get_vector_from_sentence(w, s, i)
    # should not happen but just in case 0 is returned
    if (embeddings.all() == None):
      arr.append(0)
      continue
    label, sim = find_most_similar(label_vectors, embeddings)
    lable_code = 0
    if label in label_mappings:
      label_code = label_mappings[label]
    arr.append(label_code)
  return arr

def get_labels_from_tuple_list_and_write(tuple_list, nei_labels, file_name):
  """
  for use after get_sentence_position
  """
  f = open(file_name, "w")
  arr = []
  for ind, item in enumerate(tuple_list):
    # not entity, put 0
    if (nei_labels[ind] == 0):
      arr.append(0)
      f.write("%s\n" % str(0))
      continue
    w, s, i = item
    embeddings = get_vector_from_sentence(w, s, i)
    # should not happen but just in case 0 is returned
    if (embeddings.all() == None):
      arr.append(0)
      f.write("%s\n" % str(0))
      continue
    label, sim = find_most_similar(label_vectors, embeddings)
    lable_code = 0
    if label in label_mappings:
      label_code = label_mappings[label]
      arr.append(label_code)
      f.write("%s\n" % str(label_code))
  return arr

# path to actual codes
path_to_test_18_actual = "/content/drive/MyDrive/project/NER/test_18/test_18_actual_codes.txt"
# path to the file we want to predict
path_to_18_test_input = "/content/drive/MyDrive/project/data/18_gold_labels/test.txt"
# not using this for now
path_to_test_18_predicted = "/content/drive/MyDrive/project/NER/test_18/test_18_predicted_codes.txt"

# run this cell once to create the predictions and write them to the file
# tokens_from_test_18 = load_and_parse_tokens_compiled(path_to_18_test_input)
# sentence_position_tuples = get_sentence_position(tokens_from_test_18)
# predictions_with_predicted_nei = get_labels_from_tuple_list_and_write(sentence_position_tuples, nei_labels, path_to_test_18_predicted)
# list_to_file(predictions, path_to_test_18_predicted)

# WITH ACTUAL NEIS
test_18_predicted = file_to_list(path_to_test_18_predicted)
test_18_actual = file_to_list(path_to_test_18_actual)

# ignore this cell
# from sklearn.metrics import accuracy_score
# test_18_actual_filt = [x for x in test_18_actual if x != 0]
# test_18_predicted_filt = [x for x in test_18_predicted if x != 0]
# print(accuracy_score(test_18_actual_filt, test_18_predicted_filt))

from sklearn.metrics import accuracy_score
acc = accuracy_score(test_18_predicted, test_18_actual)
print(f"test_18 NER classification: {acc}")

# WITH PREDICTED NEIs
path_to_test_18_predicted_with_nei = "/content/drive/MyDrive/project/NER/test_18/test_18_predicted_codes_with_nei.txt"
path_to_test_nei = "/content/drive/MyDrive/project/NEI_labels/expected/test_18_expected_labels.txt" # actual labels
# nei_labels_exp = file_to_list(path_to_test_nei)
# predictions_with_predicted_nei = get_labels_from_tuple_list_and_write(sentence_position_tuples, nei_labels_exp, path_to_test_18_predicted_with_nei)

test_18_predicted_with_nei = file_to_list(path_to_test_18_predicted_with_nei)
acc_final = accuracy_score(test_18_predicted_with_nei, test_18_actual)
print(f"test_18 NER classification: {acc_final}")

from sklearn.metrics import accuracy_score
test_18_predicted_with_nei = file_to_list(path_to_test_18_predicted_with_nei)
test_18_predicted_with_nei_filt = [x for x in test_18_predicted_with_nei if x != 0]
test_18_actual_filt = [x for x in test_18_actual if x != 0]

accuracy_score(test_18_actual_filt, test_18_predicted_with_nei_filt)

"""## code summary

*   `tokens = load_and_parse_tokens_compiled(filepath)` where filepath is the path to the given test file. this will return a list of tokens from the filepath
*   `tuple_list = get_sentence_position(tokens)` will return a tupe of (word, sentence, index) from a list of tokens
* `nei_labels = file_to_list(path_to_test_nei)` to load the NEI labels 
* `predictions = get_labels_from_tuple_list(tuple_list, nei_labels)` will return the predicted labels. this function also uses `label_mappings` (map of label to integer code, 0-18) and `get_vector_from_sentence(word, sentence, index)`

files needed
* "/content/drive/MyDrive/project/NER/label_vectors.json" is the `label_vectors` dictionary needed by `find_most_similar(label_vectors, to_classify)`

"""